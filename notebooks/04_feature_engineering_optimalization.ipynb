{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad13b233-e673-41b7-ad77-3799dd4190d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectorMixin\n",
    "from datetime import datetime as dt\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "50c8822c-e9f8-4655-9464-27c4c9e1eb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_og = pd.read_csv(\"../data/processed/train_postprocessed.csv\")\\ntest_og =pd.read_csv(\"../data/processed/test_postprocessed.csv\")\\n\\ntrain = train_og.copy()\\ntest = test_og.copy()'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pobieranie CZĘŚCIOWO WYCZYSZCZONYCH danych\n",
    "\n",
    "train_og = pd.read_csv(\"../data/processed/train_postprocessed.csv\")\n",
    "test_og =pd.read_csv(\"../data/processed/test_postprocessed.csv\")\n",
    "\n",
    "train = train_og.copy()\n",
    "test = test_og.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "202c9372-007c-4983-875e-d88b4614cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(train[\"SalePrice\"])\n",
    "X = train.drop(columns=[\"SalePrice\", \"Id\"])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_test = test.drop(columns=[\"Id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b7d07e8a-827f-4f52-b127-ad48733d2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowujemy preprocessing \n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "#tutaj tak naprawdę simpleimputer nie jest potrzebny, do wczytywane dane mają braki 'obrobione' (plik data.py)\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "]) \n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "        ('cat', cat_pipeline, make_column_selector(dtype_include=object))\n",
    "    ],\n",
    "    remainder='passthrough', #dodatkowo dodane, tak naprawdę nie mamy kolumn poza num i cat, więc ten krok można pominąć\n",
    "    verbose_feature_names_out=False #skracanie nowych nazw kolumn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74b45018-979b-4c04-9758-815bd7179339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Przygotowanie kroków Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2113dc89-c6c3-49fd-bedc-47a5a4a9483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usuwanie pustych kolumn - doczyszczanie\n",
    "\n",
    "class DropEmptyColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_threshold = 0.9):\n",
    "        self.drop_threshold = drop_threshold \n",
    "        self.cols_to_drop = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.cols_to_drop = X.columns[(X.isna().mean() > self.drop_threshold) |\n",
    "                                      ((X.select_dtypes(include=['int64', 'float64']) == 0).mean() > self.drop_threshold)]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        X = X.drop(columns=self.cols_to_drop, errors='ignore')\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "53be8bf9-4d26-4fe6-a5f2-8154f7a7749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtrowanie po korelacji DLA NUMERYCZNYCH\n",
    "\n",
    "class CorrelationFiltering(BaseEstimator, SelectorMixin):\n",
    "    def __init__(self, threshold=0.01):\n",
    "        self.threshold = threshold\n",
    "        self.correlations_ = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if y is None:\n",
    "            raise ValueError(\"y nie może być None dla CorrelationFiltering\")\n",
    "        \n",
    "        self.is_dataframe_ = isinstance(X, pd.DataFrame)\n",
    "        \n",
    "        if self.is_dataframe_:\n",
    "            self.numeric_cols_ = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            X_numeric = X[self.numeric_cols_]\n",
    "            values = X_numeric.values\n",
    "        else:\n",
    "            values = X\n",
    "            self.numeric_cols_ = None\n",
    "        \n",
    "        y_values = y.values if hasattr(y, 'values') else np.array(y)\n",
    "        y_values = y_values.ravel()\n",
    "        \n",
    "        for i in range(values.shape[1]):\n",
    "            col = values[:, i]\n",
    "            if np.std(col) == 0:\n",
    "                self.correlations_.append(0.0)\n",
    "            else:\n",
    "                corr = np.corrcoef(col, y_values)[0, 1]\n",
    "                self.correlations_.append(0.0 if np.isnan(corr) else corr)\n",
    "        \n",
    "        self.correlations_ = np.array(self.correlations_)\n",
    "        self.n_features_in_ = len(self.numeric_cols_) if self.is_dataframe_ else values.shape[1]\n",
    "        \n",
    "        if self.is_dataframe_:\n",
    "            self.feature_names_in_ = np.array(self.numeric_cols_)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _get_support_mask(self):\n",
    "        return np.abs(self.correlations_) > self.threshold\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.is_dataframe_:\n",
    "            X = X[self.numeric_cols_]\n",
    "        \n",
    "        mask = self._get_support_mask()\n",
    "        \n",
    "        if self.is_dataframe_:\n",
    "            selected_cols = np.array(self.numeric_cols_)[mask]\n",
    "            return X[selected_cols]\n",
    "        else:\n",
    "            return X[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cb584145-0617-4763-97ea-8e19b2fabe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class Log(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        if 'LotArea' in X.columns:\n",
    "            X['log_LotArea'] = np.log1p(X['LotArea'])\n",
    "        if 'LotFrontage' in X.columns:\n",
    "            X['log_LotFrontage'] = np.log1p(X['LotFrontage'])\n",
    "        if 'MasVnrArea' in X.columns:\n",
    "            X['log_MasVnrArea'] = np.log1p(X['MasVnrArea'])\n",
    "        if 'WoodDeckSF' in X.columns:\n",
    "            X['log_WoodDeckSF'] = np.log1p(X['WoodDeckSF'])\n",
    "        \n",
    "        return X \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "798bab7f-3b2b-4f82-aa92-5d22faf93302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "\n",
    "class AddNewColumns(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):     \n",
    "        X = X.copy()\n",
    "\n",
    "        if 'MasVnrArea' in X.columns:\n",
    "            X['HasMasVnrArea'] = (X['MasVnrArea'] > 0).astype('float64')\n",
    "\n",
    "        if 'WoodDeckSF' in X.columns:\n",
    "            X['HasWoodDeckSF'] = (X['WoodDeckSF'] > 0).astype('float64')\n",
    "\n",
    "        if 'GarageYrBlt' in X.columns:\n",
    "            X['GarageAge'] = dt.now().year - X['GarageYrBlt']\n",
    "            X = X.drop(['GarageYrBlt'], axis=1)\n",
    "\n",
    "        if all(col in X.columns for col in ['YrSold', 'YearRemodAdd', 'YearBuilt']):\n",
    "            X['AgeAtSold'] = X['YrSold'] - X['YearBuilt']\n",
    "            X['YearsSinceRemod'] = X['YrSold'] - X['YearRemodAdd'] \n",
    "            X['HouseAge'] = dt.now().year - X['YearBuilt']\n",
    "\n",
    "        if 'MoSold' in X.columns:\n",
    "            X['MoSold_sin'] = np.sin(2 * np.pi * X['MoSold'] / 12)\n",
    "            X['MoSold_cos'] = np.cos(2 * np.pi * X['MoSold'] / 12)\n",
    "            \n",
    "            high_season_months = [4, 5, 6, 7, 8]  \n",
    "            X['HighSeasonSell'] = X['MoSold'].isin(high_season_months).astype('int64')\n",
    "\n",
    "        if 'GrLivArea' in X.columns and 'TotalBsmtSF' in X.columns:\n",
    "            X['GrAndBsmtArea'] = X['GrLivArea'] + X['TotalBsmtSF']\n",
    "\n",
    "        if all(col in X.columns for col in ['FullBath', 'HalfBath', 'BsmtHalfBath', 'BsmtFullBath']):\n",
    "            X['Bathrooms'] = X['FullBath'] + X['BsmtFullBath'] + 0.5*(X['HalfBath'] + X['BsmtHalfBath'])\n",
    "\n",
    "        if all(col in X.columns for col in ['OverallQual', 'OverallCond']):\n",
    "            X['QualCondScore'] = X['OverallQual'] * X['OverallCond']\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3fd6e3db-6613-4112-a6dc-d0f992ac6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usuwanie starych kolumn \n",
    "\n",
    "class DeleteUnnecessaryColumns(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        if all(col in X.columns for col in ['HasMasVnrArea', 'MasVnrArea']):\n",
    "            X = X.drop(['MasVnrArea'], axis=1, errors='ignore')\n",
    "        \n",
    "        if all(col in X.columns for col in ['HasWoodDeckSF', 'WoodDeckSF']):\n",
    "            X = X.drop(['WoodDeckSF'], axis=1, errors='ignore')\n",
    "        \n",
    "        if all(col in X.columns for col in ['GarageYrBlt', 'GarageAge']):\n",
    "            X = X.drop(['GarageYrBlt'], axis=1, errors='ignore')\n",
    "        \n",
    "        # Wiek domu od sprzedaży \n",
    "        if all(col in X.columns for col in ['AgeAtSold', 'YearsSinceRemod', 'HouseAge', 'YrSold', 'YearRemodAdd', 'YearBuilt']):\n",
    "            X = X.drop(['YearBuilt', 'YearRemodAdd', 'YrSold'], axis=1, errors='ignore')\n",
    "        \n",
    "        # Sezonowość\n",
    "        if all(col in X.columns for col in ['MoSold', 'HighSeasonSell']):\n",
    "            X = X.drop(['MoSold'], axis=1, errors='ignore')\n",
    "        \n",
    "        # Interakcja\n",
    "        if all(col in X.columns for col in ['GrLivArea', 'TotalBsmtSF', 'GrAndBsmtArea']):\n",
    "            X = X.drop(['TotalBsmtSF'], axis=1, errors='ignore')\n",
    "        \n",
    "        if all(col in X.columns for col in ['Bathrooms', 'FullBath', 'HalfBath', 'BsmtHalfBath', 'BsmtFullBath']):\n",
    "            X = X.drop(['BsmtHalfBath', 'BsmtFullBath', 'FullBath', 'HalfBath'], axis=1, errors='ignore')\n",
    "       \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5f231-5e0d-42d7-8be6-9067b357ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#custom transformer do selekcji cech opartej na feature importance\n",
    "\n",
    "class FeatureImportanceSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.001, n_estimators=100, random_state=42):\n",
    "        self.threshold = threshold\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.feature_mask_ = None\n",
    "        self.feature_importances_ = None\n",
    "        self.n_features_in_ = None\n",
    "        self.n_features_out_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if hasattr(X, 'toarray'): \n",
    "            X = X.toarray()\n",
    "        \n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        \n",
    "        #pomocniczy GB model\n",
    "        temp_model = GradientBoostingRegressor(\n",
    "            n_estimators=self.n_estimators,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        temp_model.fit(X, y)\n",
    "        \n",
    "        self.feature_importances_ = temp_model.feature_importances_\n",
    "\n",
    "        self.feature_mask_ = self.feature_importances_ >= self.threshold\n",
    "        self.n_features_out_ = self.feature_mask_.sum()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.feature_mask_ is None:\n",
    "            raise ValueError(\"FeatureImportanceSelector must be fitted before transform!\")\n",
    "        \n",
    "        if hasattr(X, 'toarray'):  # sparse matrix\n",
    "            X = X.toarray()\n",
    "        \n",
    "        return X[:, self.feature_mask_]\n",
    "    \n",
    "    def get_selected_features(self, feature_names):\n",
    "        if self.feature_mask_ is None:\n",
    "            raise ValueError(\"FeatureImportanceSelector must be fitted first!\")\n",
    "        \n",
    "        return [name for name, selected in zip(feature_names, self.feature_mask_) if selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7b7d6-6c30-4743-95a7-1b1325d839d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "#bez optymalizacji\n",
    "gb_model = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "#z usuwaniem pustych kolumn\n",
    "gb_model_del_empty = Pipeline([\n",
    "    (\"del_empty\", DropEmptyColumns()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "#z filtrowaniem po korelacji\n",
    "gb_model_corr = Pipeline([\n",
    "    (\"corr\", CorrelationFiltering()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "#z dodawaniem nowych kolumn\n",
    "gb_model_add = Pipeline([\n",
    "    (\"add\", AddNewColumns()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "#z dodawaniem nowych i dropnięciem starych\n",
    "gb_model_add_del = Pipeline([\n",
    "    (\"add\", AddNewColumns()),\n",
    "    (\"del\", DeleteUnnecessaryColumns()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "#logarytmizowanie\n",
    "gb_model_log = Pipeline([\n",
    "    (\"log\", Log()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "#połączenie wszystkich optymalizacji (poza filtrowaniem po korelacji)\n",
    "gb_model_all = Pipeline([\n",
    "    (\"log\", Log()),\n",
    "    (\"add\", AddNewColumns()),\n",
    "    (\"del\", DeleteUnnecessaryColumns()),\n",
    "    (\"del_empty\", DropEmptyColumns()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"feature_selection\", FeatureImportanceSelector(threshold=0.00001)), #zamiana kolejności z preprocessorem\n",
    "    (\"model\", GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "#feature selection\n",
    "gb_model_fis = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"feature_selection\", FeatureImportanceSelector(threshold=0.00001)), #zamiana kolejności z preprocessorem\n",
    "    (\"model\", GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "#select k best\n",
    "gb_model_kbest = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"select\", SelectKBest(score_func=f_regression, k=50)),\n",
    "    (\"model\", GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c557917-1ff9-484e-8091-5196660c6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elastic Net\n",
    "\n",
    "#bez optymalizacji\n",
    "enet_model = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", ElasticNet(alpha=0.001, l1_ratio=0.5))\n",
    "])\n",
    "\n",
    "#z usuwaniem pustych kolumn\n",
    "enet_model_del_empty = Pipeline([\n",
    "    (\"del_empty\", DropEmptyColumns()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", ElasticNet(alpha=0.001, l1_ratio=0.5))\n",
    "])\n",
    "\n",
    "#z filtrowaniem po korelacji\n",
    "enet_model_corr = Pipeline([\n",
    "    (\"corr\", CorrelationFiltering(threshold=0.001)),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", ElasticNet(alpha=0.001, l1_ratio=0.5))\n",
    "])\n",
    "\n",
    "#z dodawaniem nowych kolumn\n",
    "enet_model_add = Pipeline([\n",
    "    (\"add\", AddNewColumns()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", ElasticNet(alpha=0.001, l1_ratio=0.5))\n",
    "])\n",
    "\n",
    "#z dodawaniem nowych i dropnięciem starych\n",
    "enet_model_add_del = Pipeline([\n",
    "    (\"add\", AddNewColumns()),\n",
    "    (\"del\", DeleteUnnecessaryColumns()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", ElasticNet(alpha=0.001, l1_ratio=0.5))\n",
    "])\n",
    "\n",
    "#logarytmizowanie\n",
    "enet_model_log = Pipeline([\n",
    "    (\"log\", Log()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", ElasticNet(alpha=0.001, l1_ratio=0.5))\n",
    "])\n",
    "\n",
    "#połączenie wszystkich optymalizacji\n",
    "enet_model_all = Pipeline([\n",
    "    (\"log\", Log()),\n",
    "    (\"add\", AddNewColumns()),\n",
    "    (\"del\", DeleteUnnecessaryColumns()),\n",
    "    (\"corr\", CorrelationFiltering()),\n",
    "    (\"del_empty\", DropEmptyColumns()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", ElasticNet(alpha=0.001, l1_ratio=0.5))\n",
    "])\n",
    "\n",
    "#połączenie tylko log, add\n",
    "enet_model_la = Pipeline([\n",
    "    (\"log\", Log()),\n",
    "    (\"add\", AddNewColumns()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", ElasticNet(alpha=0.001, l1_ratio=0.5))\n",
    "])\n",
    "\n",
    "#select k best\n",
    "enet_model_kbest = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"select\", SelectKBest(score_func=f_regression, k=50)),\n",
    "    (\"model\", ElasticNet(alpha=0.001, l1_ratio=0.5))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e877e-07b3-4027-b62e-3d2e7f21ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elastic Net (liniowy)\n",
    "\n",
    "#bez optymalizacji\n",
    "enet_model.fit(X_train, y_train)\n",
    "pred_enet = enet_model.predict(X_val)\n",
    "\n",
    "#z usuwaniem pustych kolumn\n",
    "enet_model_del_empty.fit(X_train, y_train)\n",
    "pred_enet_del_empty = enet_model_del_empty.predict(X_val)\n",
    "\n",
    "#z filtrowaniem po korelacji\n",
    "enet_model_corr.fit(X_train, y_train)\n",
    "pred_enet_corr = enet_model_corr.predict(X_val)\n",
    "\n",
    "#z dodawaniem nowych kolumn\n",
    "enet_model_add.fit(X_train, y_train)\n",
    "pred_enet_add = enet_model_add.predict(X_val)\n",
    "\n",
    "#z dodawaniem nowych i dropnięciem starych\n",
    "enet_model_add_del.fit(X_train, y_train)\n",
    "pred_enet_add_del = enet_model_add_del.predict(X_val)\n",
    "\n",
    "#logarytmizowanie\n",
    "enet_model_log.fit(X_train, y_train)\n",
    "pred_enet_log = enet_model_log.predict(X_val)\n",
    "\n",
    "#połączenie wszystkich optymalizacji\n",
    "enet_model_all.fit(X_train, y_train)\n",
    "pred_enet_all = enet_model_all.predict(X_val)\n",
    "\n",
    "#połączenie tylko log, add, drop_empty\n",
    "enet_model_la.fit(X_train, y_train)\n",
    "pred_enet_la = enet_model_la.predict(X_val)\n",
    "\n",
    "#SelectKBest\n",
    "enet_model_kbest.fit(X_train, y_train)\n",
    "pred_enet_kbest = enet_model_kbest.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee40394-8d3b-4227-a517-c6e31f117e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting (drzewiasty)\n",
    "\n",
    "#bez optymalizacji \n",
    "gb_model.fit(X_train, y_train)\n",
    "pred_gb = gb_model.predict(X_val)\n",
    "\n",
    "#z usuwaniem pustych kolumn\n",
    "gb_model_del_empty.fit(X_train, y_train)\n",
    "pred_gb_del_empty = gb_model_del_empty.predict(X_val)\n",
    "\n",
    "#z dodawaniem nowych kolumn\n",
    "gb_model_add.fit(X_train, y_train)\n",
    "pred_gb_add = gb_model_add.predict(X_val)\n",
    "\n",
    "#z dodawaniem nowych i dropnięciem starych\n",
    "gb_model_add_del.fit(X_train, y_train)\n",
    "pred_gb_add_del = gb_model_add_del.predict(X_val)\n",
    "\n",
    "#logarytmizowanie\n",
    "gb_model_log.fit(X_train, y_train)\n",
    "pred_gb_log = gb_model_log.predict(X_val)\n",
    "\n",
    "#połączenie wszystkich optymalizacji\n",
    "gb_model_all.fit(X_train, y_train)\n",
    "pred_gb_all = gb_model_all.predict(X_val)\n",
    "\n",
    "#fis\n",
    "gb_model_fis.fit(X_train, y_train)\n",
    "pred_gb_fis = gb_model_fis.predict(X_val)\n",
    "\n",
    "#SelectKBest\n",
    "gb_model_kbest.fit(X_train, y_train)\n",
    "pred_gb_kbest = gb_model_kbest.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2c078-235f-41ef-b3f0-c7fa0789fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Lista modeli i ich predykcji\n",
    "models = ['ElasticNet', 'Gradient Boosting', 'ENET_drop_empty', 'GB_drop_empty', 'ENET_corr',  \n",
    "         'ENET_add', 'GB_add', 'ENET_add_del', 'GB_add_del', 'ENET_log', 'GB_log',\n",
    "         'ENET_all', 'GB_all', 'ENET_log_add', 'GB_feature_selection', 'ENET_SelectKBest',\n",
    "         'GB_SelectKBest']\n",
    "predictions = [pred_enet, pred_gb, pred_enet_del_empty, pred_gb_del_empty, pred_enet_corr, \n",
    "              pred_enet_add, pred_enet_add_del, pred_gb_add, pred_gb_add_del, pred_enet_log, pred_gb_log,\n",
    "              pred_enet_all, pred_gb_all, pred_enet_la, pred_gb_fis, pred_enet_kbest,\n",
    "              pred_gb_kbest]\n",
    "\n",
    "# obliczanie metryk\n",
    "results = []\n",
    "for name, pred in zip(models, predictions):\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "    r2 = r2_score(y_val, pred)\n",
    "    results.append([name, rmse, mae, r2])\n",
    "\n",
    "# tworzenie DataFrame i sortowanie po R2\n",
    "df_results = pd.DataFrame(results, columns=['Model', 'RMSE', 'MAE', 'R2'])\n",
    "df_results = df_results.sort_values(by='R2', ascending=False).reset_index(drop=True)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589d864-def0-43c5-b30d-926336dc5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wybór najlepszych pipeline'ów na podstawie metryk\n",
    "\n",
    "#normalizowanie metryk i obliczanie średniej\n",
    "df_results['RMSE_norm'] = (df_results['RMSE'].max() - df_results['RMSE']) / (df_results['RMSE'].max() - df_results['RMSE'].min())\n",
    "df_results['MAE_norm'] = (df_results['MAE'].max() - df_results['MAE']) /  (df_results['MAE'].max() - df_results['MAE'].min())\n",
    "df_results['R2_norm'] = (df_results['R2'] - df_results['R2'].min()) / (df_results['R2'].max() - df_results['R2'].min())\n",
    "\n",
    "#średnia ważona\n",
    "df_results['Combined_Score'] = (\n",
    "    0.5 * df_results['RMSE_norm'] +  \n",
    "    0.3 * df_results['MAE_norm'] +   \n",
    "    0.2 * df_results['R2_norm']  \n",
    ")\n",
    "\n",
    "df_results.sort_values('Combined_Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe85bbb-f572-4b10-862c-419ce7e067e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zapis najlepszych piplienów\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(enet_model_la, \"best_enet_pipeline.pkl\")\n",
    "joblib.dump(gb_model_add, \"best_gb_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b12521-5374-4076-92f6-e9e46f1a880d",
   "metadata": {},
   "source": [
    "# Optymalizacja modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69988167-b91d-40fd-a184-6235bc80fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet - regularyzacja i GridSearchCV\n",
    "\n",
    "enet_param_grid = {\n",
    "    \"model__alpha\": [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"model__l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "enet_grid = GridSearchCV(\n",
    "    estimator=enet_model_la,\n",
    "    param_grid=enet_param_grid,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "enet_grid.fit(X_train, y_train)\n",
    "\n",
    "best_enet_pipeline = enet_grid.best_estimator_\n",
    "\n",
    "print(\"ElasticNet BEST params:\", enet_grid.best_params_)\n",
    "print(\"ElasticNet BEST CV RMSE:\", -enet_grid.best_score_)\n",
    "\n",
    "\"\"\"\"ElasticNet BEST params: {'model__alpha': 0.001, 'model__l1_ratio': 0.9}\n",
    "ElasticNet BEST CV RMSE: 0.11038763345351361\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7440c05-c3e0-4d79-991a-4a7ddff11a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting - optymailziacja hyperparametrów Grid\n",
    "\n",
    "gb_param_grid = {\n",
    "    \"model__n_estimators\": [200, 300, 500],\n",
    "    \"model__learning_rate\": [0.03, 0.05, 0.1],\n",
    "    \"model__max_depth\": [2, 3, 4],\n",
    "    \"model__subsample\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "gb_grid = GridSearchCV(\n",
    "    estimator=gb_model_add,\n",
    "    param_grid=gb_param_grid,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gb_grid.fit(X_train, y_train)\n",
    "\n",
    "best_gb_pipeline = gb_grid.best_estimator_\n",
    "\n",
    "print(\"GB BEST params:\", gb_grid.best_params_)\n",
    "print(\"GB BEST CV RMSE:\", -gb_grid.best_score_)\n",
    "\n",
    "\"\"\"GB BEST params: {'model__learning_rate': 0.05, 'model__max_depth': 4, 'model__n_estimators': 500, 'model__subsample': 0.8}\n",
    "GB BEST CV RMSE: 0.11402089405126632\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c3a6e-557c-48f6-b673-5bf5a067a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(model, X_train, y_train, X_val, y_val, stage, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Etap\": stage,\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_val, y_pred_val)),\n",
    "        \"R2\": r2_score(y_val, y_pred_val)\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "enet_bazowy = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", ElasticNet(alpha=0.001, l1_ratio=0.5))\n",
    "])\n",
    "results.append(eval_model(enet_bazowy, X_train, y_train, X_val, y_val, \"Bazowy\", \"ElasticNet\"))\n",
    "\n",
    "# Pipeline - z feature engineering (log + add)\n",
    "results.append(eval_model(enet_model_la, X_train, y_train, X_val, y_val, \"Pipeline\", \"ElasticNet\"))\n",
    "\n",
    "# Pipeline + Tuning - najlepszy model po GridSearchCV\n",
    "results.append(eval_model(best_enet_pipeline, X_train, y_train, X_val, y_val, \"Pipeline + Tuning\", \"ElasticNet\"))\n",
    "\n",
    "gb_bazowy = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "results.append(eval_model(gb_bazowy, X_train, y_train, X_val, y_val, \"Bazowy\", \"GradientBoosting\"))\n",
    "\n",
    "# Pipeline - z feature engineering (add)\n",
    "results.append(eval_model(gb_model_add, X_train, y_train, X_val, y_val, \"Pipeline\", \"GradientBoosting\"))\n",
    "\n",
    "# Pipeline + Tuning - najlepszy model po GridSearchCV\n",
    "results.append(eval_model(best_gb_pipeline, X_train, y_train, X_val, y_val, \"Pipeline + Tuning\", \"GradientBoosting\"))\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_rmse = df_results.pivot(index='Model', columns='Etap', values='RMSE')\n",
    "df_r2 = df_results.pivot(index='Model', columns='Etap', values='R2')\n",
    "\n",
    "df = pd.concat([df_rmse.add_suffix(\" (RMSE)\"), df_r2.add_suffix(\" (R2)\")], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fda2a-8685-4d3a-9f88-ad2122fbcb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wykres porównawczy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Kolory: niebieski i pomarańczowy dla etapów\n",
    "palette = [\"#1f77b4\", \"#ff7f0e\", \"#aec7e8\"]  # Bazowy, Pipeline, Pipeline + Tuning\n",
    "\n",
    "# Przygotowanie danych\n",
    "df_plot = df_results.melt(id_vars=['Model', 'Etap'], value_vars=['RMSE', 'R2'], \n",
    "                          var_name='Metric', value_name='Score')\n",
    "\n",
    "# Oddzielne wykresy dla RMSE i R2\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
    "\n",
    "# RMSE\n",
    "sns.barplot(data=df_plot[df_plot['Metric']=='RMSE'], x='Model', y='Score', hue='Etap', palette=palette, ax=axes[0])\n",
    "axes[0].set_title('RMSE modeli')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].set_ylim(0.008, None) \n",
    "axes[0].legend(title='Etap', loc='upper right')\n",
    "\n",
    "# R2\n",
    "sns.barplot(data=df_plot[df_plot['Metric']=='R2'], x='Model', y='Score', hue='Etap', palette=palette, ax=axes[1])\n",
    "axes[1].set_title('R² modeli')\n",
    "axes[1].set_ylabel('R²')\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].set_ylim(0.84, 0.9) \n",
    "axes[1].legend(title='Etap', loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98213aa-bd98-4d7d-afa1-e713392eacf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalne predykcje na zbiorze testowym\n",
    "\n",
    "#Gradient Boosting\n",
    "final_predictions_gb = best_gb_pipeline.predict(X_test)\n",
    "\n",
    "#ElasticNet\n",
    "final_predictions_enet = best_enet_pipeline.predict(X_test)\n",
    "\n",
    "#odwrócenie log \n",
    "# SalePrice = np.expm1(y)\n",
    "final_prices_gb = np.expm1(final_predictions_gb)\n",
    "final_prices_enet = np.expm1(final_predictions_enet)\n",
    "\n",
    "submission_gb = pd.DataFrame({\n",
    "    'Id': test_og['Id'],\n",
    "    'SalePrice': final_prices_gb\n",
    "})\n",
    "\n",
    "submission_enet = pd.DataFrame({\n",
    "    'Id': test_og['Id'],\n",
    "    'SalePrice': final_prices_enet\n",
    "})\n",
    "\n",
    "submission_gb.to_csv('../data/predictions/submission_gb_tuned.csv', index=False)\n",
    "submission_enet.to_csv('../data/predictions/submission_enet_tuned.csv', index=False)\n",
    "\n",
    "print(\"Gradient Boosting submission:\")\n",
    "print(submission_gb.head())\n",
    "print(f\"\\nŚrednia przewidywana cena: ${final_prices_gb.mean():.2f}\")\n",
    "print(f\"Mediana: ${np.median(final_prices_gb):.2f}\")\n",
    "\n",
    "print(\"\\nElasticNet submission:\")\n",
    "print(submission_enet.head())\n",
    "print(f\"\\nŚrednia przewidywana cena: ${final_prices_enet.mean():.2f}\")\n",
    "print(f\"Mediana: ${np.median(final_prices_enet):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
